{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhT97QmjpP046FzibLlYMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BayramovaNazrin/Elliptic2-ML/blob/main/02_node_embeddings_to_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biWjrZYwqupa",
        "outputId": "93235d3d-d612-49d1-f0ca-3276db345464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading Node2Vec embeddings from temporary storage...\n",
            "Loaded 716 embeddings.\n",
            "2. Loading node mapping (clId -> subgraph_id) from Drive...\n",
            "Loaded 444521 node mappings.\n",
            "3. Loading connected components labels (subgraph_id -> label) from Drive...\n",
            "Loaded 121810 component labels.\n",
            "\n",
            "4. Merging datasets...\n",
            "Nodes after first merge: 716\n",
            "Nodes after final merge: 716\n",
            "5. Filtering out nodes with 'unknown' labels...\n",
            "Remaining nodes for classification: 696\n",
            "\n",
            "6. Converting string embeddings to feature columns...\n",
            "Expanded embedding dimension: 64\n",
            "\n",
            "7. Saving the final dataset to CSV...\n",
            "Final dataset shape: (696, 66)\n",
            "Successfully saved the final dataset to /content/node_dataset.csv.\n",
            "\n",
            "Script finished successfully! The data is now ready for machine learning.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "import ast # Used for safely converting string representation of a list to an actual list\n",
        "\n",
        "# --- Configuration and Setup ---\n",
        "# Define file paths\n",
        "EMBEDDINGS_FILE = '/content/drive/MyDrive/elliptic2/node_embeddings.csv'\n",
        "NODES_MAPPING_FILE = '/content/drive/MyDrive/elliptic2/nodes.csv'\n",
        "COMPONENTS_LABELS_FILE = '/content/drive/MyDrive/elliptic2/connected_components.csv'\n",
        "OUTPUT_FILE = '/content/node_dataset.csv'\n",
        "\n",
        "# Define assumed embedding dimension\n",
        "EMBEDDING_DIM = 64\n",
        "LABEL_COLUMN_NAME = 'label' # Standardized final label column name\n",
        "\n",
        "# --- 1. Load the Node2Vec embeddings ---\n",
        "print(\"1. Loading Node2Vec embeddings from temporary storage...\")\n",
        "try:\n",
        "    # Read the embeddings file\n",
        "    df_embeddings = pd.read_csv(EMBEDDINGS_FILE)\n",
        "    df_embeddings.rename(columns={'node_id': 'clId'}, inplace=True)\n",
        "    print(f\"Loaded {len(df_embeddings)} embeddings.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {EMBEDDINGS_FILE} not found. Please run the previous script first.\")\n",
        "    # Exit gracefully if the file is missing\n",
        "    # You may need to mount Drive here if the previous script was not run.\n",
        "    drive.mount('/content/drive')\n",
        "    exit()\n",
        "\n",
        "# --- 2. Load the nodes mapping (clId -> subgraph_id) ---\n",
        "print(\"2. Loading node mapping (clId -> subgraph_id) from Drive...\")\n",
        "try:\n",
        "    # We only need 'clId' (node ID) and 'subgraph_id' (which you called ccId)\n",
        "    df_nodes = pd.read_csv(NODES_MAPPING_FILE, usecols=['clId', 'ccId'])\n",
        "    print(f\"Loaded {len(df_nodes)} node mappings.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {NODES_MAPPING_FILE} not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Load the connected components labels (subgraph_id -> ccLabel) ---\n",
        "print(\"3. Loading connected components labels (subgraph_id -> label) from Drive...\")\n",
        "try:\n",
        "    # We only need 'subgraph_id' and 'ccLabel'\n",
        "    df_labels = pd.read_csv(COMPONENTS_LABELS_FILE, usecols=['ccId', 'ccLabel'])\n",
        "    print(f\"Loaded {len(df_labels)} component labels.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {COMPONENTS_LABELS_FILE} not found. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Merge the datasets ---\n",
        "print(\"\\n4. Merging datasets...\")\n",
        "\n",
        "# First, merge embeddings with the node-to-subgraph mapping (df_embeddings + df_nodes)\n",
        "df_merged = pd.merge(df_embeddings, df_nodes, on='clId', how='left')\n",
        "print(f\"Nodes after first merge: {len(df_merged)}\")\n",
        "\n",
        "# Second, merge with the subgraph-to-label mapping (df_merged + df_labels)\n",
        "# Note: 'subgraph_id' in df_nodes is the 'ccId' reference\n",
        "df_final = pd.merge(df_merged, df_labels, on='ccId', how='left')\n",
        "print(f\"Nodes after final merge: {len(df_final)}\")\n",
        "\n",
        "# Cleanup intermediate DataFrames\n",
        "del df_embeddings, df_nodes, df_labels, df_merged\n",
        "\n",
        "# --- 5. Remove nodes with unknown labels (keeping only 'licit' and 'illicit') ---\n",
        "print(\"5. Filtering out nodes with 'unknown' labels...\")\n",
        "\n",
        "# Filter the dataset\n",
        "df_final = df_final[df_final['ccLabel'].isin(['licit', 'illicit'])]\n",
        "print(f\"Remaining nodes for classification: {len(df_final)}\")\n",
        "\n",
        "# Rename the label column to the standardized name\n",
        "df_final.rename(columns={'ccLabel': LABEL_COLUMN_NAME}, inplace=True)\n",
        "\n",
        "# --- 6. Convert the 'embedding' column and expand into separate columns ---\n",
        "print(\"\\n6. Converting string embeddings to feature columns...\")\n",
        "\n",
        "# A. Convert string representation of list to actual list/array\n",
        "# We use the literal_eval from the 'ast' module for safe string evaluation.\n",
        "df_final['embedding'] = df_final['embedding'].apply(ast.literal_eval)\n",
        "\n",
        "# B. Expand the list of floats into separate columns\n",
        "# Create a DataFrame from the list of embeddings\n",
        "df_emb_expanded = pd.DataFrame(\n",
        "    df_final['embedding'].to_list(),\n",
        "    index=df_final.index, # Crucial to align with the original DataFrame index\n",
        "    columns=[f'emb_{i}' for i in range(EMBEDDING_DIM)]\n",
        ")\n",
        "\n",
        "print(f\"Expanded embedding dimension: {df_emb_expanded.shape[1]}\")\n",
        "\n",
        "\n",
        "# C. Concatenate the expanded embeddings with the original DataFrame\n",
        "df_final = pd.concat([df_final.drop(columns=['embedding', 'ccId']), df_emb_expanded], axis=1)\n",
        "\n",
        "# --- 7. Save the resulting dataset ---\n",
        "print(\"\\n7. Saving the final dataset to CSV...\")\n",
        "\n",
        "# Reorder columns to place node_id and label first for clarity\n",
        "column_order = ['clId'] + [f'emb_{i}' for i in range(EMBEDDING_DIM)] + [LABEL_COLUMN_NAME]\n",
        "df_final = df_final[column_order]\n",
        "\n",
        "# Save the final cleaned, ready-to-use dataset\n",
        "df_final.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"Final dataset shape: {df_final.shape}\")\n",
        "print(f\"Successfully saved the final dataset to {OUTPUT_FILE}.\")\n",
        "print(\"\\nScript finished successfully! The data is now ready for machine learning.\")"
      ]
    }
  ]
}