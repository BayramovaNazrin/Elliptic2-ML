{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy5p85Gsbk70NHEckj5VDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    
    },
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BayramovaNazrin/Elliptic2-ML/blob/main/01_node2vec_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uk7yBs9kX6Rl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51c8c0b-1e0f-498e-9d3c-28e44adef325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   ccId ccLabel\n",
            "0     0   licit\n",
            "1     1   licit\n",
            "2     2   licit\n",
            "3     3   licit\n",
            "4     4   licit\n",
            "        clId    ccId\n",
            "0  515498410   41121\n",
            "1  630366534   27974\n",
            "2  903790945  108020\n",
            "3  449108887    6544\n",
            "4  877994419   27234\n",
            "       clId1      clId2      txId\n",
            "0  753456251  753456254  29911377\n",
            "1  756183927  759736869     51855\n",
            "2  623574254  622935561  27784128\n",
            "3  751464959  751464964     76668\n",
            "4  751464834  751464959   2592471\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "connected_components_file = '/content/drive/MyDrive/elliptic2/connected_components.csv'\n",
        "nodes_file = '/content/drive/MyDrive/elliptic2/nodes.csv'\n",
        "edges_file = '/content/drive/MyDrive/elliptic2/edges.csv'\n",
        "\n",
        "cc = pd.read_csv(connected_components_file)\n",
        "print(cc.head())\n",
        "\n",
        "nodes = pd.read_csv(nodes_file)\n",
        "print(nodes.head())\n",
        "\n",
        "edges = pd.read_csv(edges_file)\n",
        "print(edges.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "k2f6yXJpgL6T",
        "outputId": "7cd6b07e-d244-4097-e219-608580907549"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Collecting gensim<5.0.0,>=4.3.0 (from node2vec)\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (1.5.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from node2vec) (3.6.1)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from node2vec)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from node2vec) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (2.0.1)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, gensim, node2vec\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.4.0 node2vec-0.5.0 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "98295853bf08488f84b35d086fa14adc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "from google.colab import drive\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "print(\"1. Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the directory where the CSV files are located in Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive/elliptic2/'\n",
        "\n",
        "# Define input file paths\n",
        "CONNECTED_COMPONENTS_FILE = os.path.join(DRIVE_PATH, 'connected_components.csv')\n",
        "NODES_FILE = os.path.join(DRIVE_PATH, 'nodes.csv')\n",
        "EDGES_FILE = os.path.join(DRIVE_PATH, 'edges.csv')\n",
        "\n",
        "# Define output file paths\n",
        "NODES_SAMPLE_FILE = '/content/nodes_sample.csv'\n",
        "EDGES_SAMPLE_FILE = '/content/edges_sample.csv'\n",
        "NODE_EMBEDDINGS_FILE = '/content/node_embeddings.csv'\n",
        "\n",
        "# A random seed for reproducibility in sampling\n",
        "random.seed(42)\n",
        "\n",
        "# --- 2. Read the CSV files (using an iterator for connected_components) ---\n",
        "\n",
        "print(\"\\n2. Reading connected_components.csv...\")\n",
        "try:\n",
        "    df_components = pd.read_csv(CONNECTED_COMPONENTS_FILE)\n",
        "    print(f\"Read {len(df_components)} rows from connected_components.csv.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: connected_components.csv not found at {CONNECTED_COMPONENTS_FILE}\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Randomly select 200 subgraphs from connected_components.csv ---\n",
        "TARGET_SUBGRAPHS = 200\n",
        "\n",
        "# Get all unique subgraph IDs\n",
        "all_subgraph_ids = df_components['ccId'].unique()\n",
        "\n",
        "# Sample the desired number of unique subgraph IDs\n",
        "if len(all_subgraph_ids) >= TARGET_SUBGRAPHS:\n",
        "    selected_subgraph_ids = random.sample(list(all_subgraph_ids), TARGET_SUBGRAPHS)\n",
        "    print(f\"3. Randomly selected {TARGET_SUBGRAPHS} subgraphs.\")\n",
        "else:\n",
        "    # If the total number of subgraphs is less than 200, select all of them\n",
        "    selected_subgraph_ids = all_subgraph_ids\n",
        "    TARGET_SUBGRAPHS = len(all_subgraph_ids)\n",
        "    print(f\"3. Selected all {TARGET_SUBGRAPHS} available subgraphs.\")\n",
        "\n",
        "print(\"\\nGetting sample node IDs from nodes.csv based on selected subgraphs...\")\n",
        "df_nodes_full = pd.read_csv(NODES_FILE, usecols=['clId', 'ccId'])\n",
        "sample_node_ids = set(df_nodes_full[df_nodes_full['ccId'].isin(selected_subgraph_ids)]['clId'].unique())\n",
        "del df_nodes_full # Free up memory after use\n",
        "\n",
        "print(f\"Total nodes in the selected subgraphs: {len(sample_node_ids)}\")\n",
        "\n",
        "\n",
        "# --- 4. Filter nodes.csv and edges.csv for the selected subgraphs ---\n",
        "\n",
        "# A. Filter nodes.csv\n",
        "print(\"\\n4.A. Filtering nodes.csv...\")\n",
        "chunksize = 100000\n",
        "nodes_sample_list = []\n",
        "for chunk in pd.read_csv(NODES_FILE, chunksize=chunksize):\n",
        "    # 'clId' is the node ID column\n",
        "    filtered_chunk = chunk[chunk['clId'].isin(sample_node_ids)]\n",
        "    nodes_sample_list.append(filtered_chunk)\n",
        "    # Clear memory of the chunk\n",
        "    del chunk\n",
        "\n",
        "# Concatenate and save the sampled nodes\n",
        "df_nodes_sample = pd.concat(nodes_sample_list)\n",
        "df_nodes_sample.to_csv(NODES_SAMPLE_FILE, index=False)\n",
        "print(f\"Saved {len(df_nodes_sample)} nodes to {NODES_SAMPLE_FILE}.\")\n",
        "# Clear the list of chunks to free memory\n",
        "del nodes_sample_list\n",
        "\n",
        "# B. Filter edges.csv\n",
        "print(\"\\n4.B. Filtering edges.csv...\")\n",
        "# Edges.csv can also be very large. We use chunking and filter edges where BOTH clId1 and clId2\n",
        "# are in our list of sampled nodes (sample_node_ids).\n",
        "edges_sample_list = []\n",
        "for chunk in pd.read_csv(EDGES_FILE, chunksize=chunksize):\n",
        "    # 'clId1' and 'clId2' are the source and target node ID columns\n",
        "    # We must ensure both nodes belong to the sampled set\n",
        "    mask = chunk['clId1'].isin(sample_node_ids) & chunk['clId2'].isin(sample_node_ids)\n",
        "    filtered_chunk = chunk[mask]\n",
        "    edges_sample_list.append(filtered_chunk)\n",
        "    # Clear memory of the chunk\n",
        "    del chunk\n",
        "\n",
        "# Concatenate and save the sampled edges\n",
        "df_edges_sample = pd.concat(edges_sample_list)\n",
        "df_edges_sample.to_csv(EDGES_SAMPLE_FILE, index=False)\n",
        "print(f\"Saved {len(df_edges_sample)} edges to {EDGES_SAMPLE_FILE}.\")\n",
        "# Clear the list of chunks to free memory\n",
        "del edges_sample_list\n",
        "\n",
        "# --- 5. Create a NetworkX graph from edges_sample.csv ---\n",
        "print(\"\\n5. Creating NetworkX graph...\")\n",
        "\n",
        "# We use the filtered DataFrame of edges\n",
        "# Create an undirected graph from the source ('clId1') and target ('clId2') columns\n",
        "# We can specify the columns to use to minimize memory usage of the graph object\n",
        "G = nx.from_pandas_edgelist(\n",
        "    df_edges_sample,\n",
        "    source='clId1',\n",
        "    target='clId2',\n",
        "    create_using=nx.Graph() # Use nx.DiGraph() if the graph is directed\n",
        ")\n",
        "print(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "# Release memory from the sample DataFrames\n",
        "del df_components, df_nodes_sample, df_edges_sample\n",
        "\n",
        "# --- 6. Train Node2Vec embeddings on the graph ---\n",
        "print(\"\\n6. Training Node2Vec embeddings...\")\n",
        "\n",
        "# Node2Vec Hyperparameters (using defaults for simplicity)\n",
        "# dimensions: Embedding dimension (e.g., 64, 128)\n",
        "# walk_length: Maximum length of a random walk (e.g., 30)\n",
        "# num_walks: Number of random walks per node (e.g., 10)\n",
        "# workers: Number of parallel jobs (use more for faster computation)\n",
        "DIMENSIONS = 64\n",
        "WALK_LENGTH = 30\n",
        "NUM_WALKS = 10\n",
        "P = 1 # Return hyperparameter\n",
        "Q = 1 # In-out hyperparameter\n",
        "WORKERS = 4 # Adjust based instance's vCPUs\n",
        "\n",
        "# Pre-compute probabilities and generate walks\n",
        "node2vec = Node2Vec(\n",
        "    G,\n",
        "    dimensions=DIMENSIONS,\n",
        "    walk_length=WALK_LENGTH,\n",
        "    num_walks=NUM_WALKS,\n",
        "    p=P,\n",
        "    q=Q,\n",
        "    workers=WORKERS,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "\n",
        "model = node2vec.fit(\n",
        "    window=10,\n",
        "    min_count=1,\n",
        "    batch_words=4\n",
        ")\n",
        "\n",
        "# --- 7. Save the embeddings to node_embeddings.csv ---\n",
        "print(\"\\n7. Saving embeddings...\")\n",
        "\n",
        "# Extract embeddings for all nodes in the graph\n",
        "node_ids = list(G.nodes())\n",
        "embedding_vectors = [model.wv[str(node_id)] for node_id in node_ids]\n",
        "\n",
        "# Create a DataFrame for the output\n",
        "# The embedding vector is converted to a string representation for CSV\n",
        "df_embeddings = pd.DataFrame({\n",
        "    'node_id': node_ids,\n",
        "    # Convert the numpy array to a list/string for easy CSV storage\n",
        "    'embedding': [vector.tolist() for vector in embedding_vectors]\n",
        "})\n",
        "\n",
        "# Save the final embeddings DataFrame\n",
        "df_embeddings.to_csv(NODE_EMBEDDINGS_FILE, index=False)\n",
        "\n",
        "print(f\"Successfully saved {len(df_embeddings)} embeddings to {NODE_EMBEDDINGS_FILE}.\")\n",
        "print(\"Script finished successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "15f42aa1d62243c39b82bae5b019df73",
            "c6a94e4ca2c44dc2a5aea21c0807d9bf",
            "074719d373e34ad2b48b9b850ca1884b",
            "56e20c9f84b84d7f8d5a8305239a652a",
            "074526faea59492eb4470058d68ca6e0",
            "11890f77a24a4c799d8c92c87dcc41d8",
            "949248e5aad64253b7067b431bca914d",
            "08ab5b77e8bb49ee99d066d4c0a3ed80",
            "d8b7bbc1431a4e60bc858e5a19af7968",
            "8339d9c16ed94eedaa6b0e8a441c59f3",
            "f30f8346e6e648c9922188c1de25f7f9"
          ]
        },
        "id": "i9A8gUjjbjaH",
        "outputId": "6a1ff024-ada9-4305-97c2-2050f3a70afe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "2. Reading connected_components.csv...\n",
            "Read 121810 rows from connected_components.csv.\n",
            "3. Randomly selected 200 subgraphs.\n",
            "\n",
            "Getting sample node IDs from nodes.csv based on selected subgraphs...\n",
            "Total nodes in the selected subgraphs: 716\n",
            "\n",
            "4.A. Filtering nodes.csv...\n",
            "Saved 716 nodes to /content/nodes_sample.csv.\n",
            "\n",
            "4.B. Filtering edges.csv...\n",
            "Saved 565 edges to /content/edges_sample.csv.\n",
            "\n",
            "5. Creating NetworkX graph...\n",
            "Graph created with 716 nodes and 529 edges.\n",
            "\n",
            "6. Training Node2Vec embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/716 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15f42aa1d62243c39b82bae5b019df73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7. Saving embeddings...\n",
            "Successfully saved 716 embeddings to /content/node_embeddings.csv.\n",
            "Script finished successfully!\n"
          ]
        }
      ]
    }
  ]
}
